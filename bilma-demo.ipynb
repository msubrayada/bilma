{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31348be1-ab0e-47e7-84fb-a5724c019287",
   "metadata": {},
   "source": [
    "# Bilma tutorial\n",
    "## import bilma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9386b41-b377-4aa8-aabd-164ca29d17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bilma import bilma_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc9b96-3d4e-4619-b305-24f8c86568c2",
   "metadata": {},
   "source": [
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d1da7a-2d9b-46b3-9647-4d1ffc14221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bilma_model.load(\"models-final/bilma_small_MX_epoch-1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64ff61-6788-4bf2-8f9f-c3fd14f8f3c4",
   "metadata": {},
   "source": [
    "## Show the model\n",
    "\n",
    "The model structure can be showed with `model.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6385223-cae9-4fd4-821d-00309f20a2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "capt_input (InputLayer)      [(None, 280)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 280, 512)          14860800  \n",
      "_________________________________________________________________\n",
      "encoder_5 (Encoder)          (None, 280, 512)          9456640   \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 280, 29025)        14889825  \n",
      "=================================================================\n",
      "Total params: 39,207,265\n",
      "Trainable params: 39,207,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5259d-c32f-48de-8d98-cf71fc09079a",
   "metadata": {},
   "source": [
    "The model's input is a tensor of size `(bs, 280)` where bs is the batch size. Each sequence contains the id's of the tokens on the input text with a maximum length of 280."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef750580-370f-4774-b54d-265a4b117e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 280) dtype=float32 (created by layer 'capt_input')>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b2a88-8d9c-43cf-b0c6-c3d0b7a25fbe",
   "metadata": {},
   "source": [
    "The output is a tensor of size `(bs, 280, 29025)` where 29025 is the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e142bec7-5272-4036-8457-db617c10dda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 280, 29025) dtype=float32 (created by layer 'dense_37')>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f55f3-0f64-4bfa-bd53-dfe5a30c8db7",
   "metadata": {},
   "source": [
    "# Test MLM on new text\n",
    "\n",
    "To feed the model with text we need to tranform it to tokens with a tokenizer. `bilma_model.tokenizer` returns a tokenizer to do that, we just need to pass the vocabulary file and the max length of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e1d410-2710-4f97-83c3-7e3bfc17e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bilma_model.tokenizer(vocab_file=\"d:/data/twitts/vocab_file_ALL.txt\", max_length=280)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ede604-81cb-4b87-bdb9-8776ab1144e3",
   "metadata": {},
   "source": [
    "After that, we can use the `tokenize` function to transform the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32213681-fb4d-4456-aa3b-81a4bfcad4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Tenemos tres días sin internet ni señal de celular en el pueblo.\",\n",
    "         \"Incomunicados en el siglo XXI tampoco hay servicio de telefonía fija\",\n",
    "         \"Vamos a comer unos tacos\"]\n",
    "tweet = tokenizer.tokenize(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e82d40-de6e-4565-baf7-31d874a5b51e",
   "metadata": {},
   "source": [
    "Now, tweet contain the tokenized text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d5a3f3-c721-4663-9202-d8542babb118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 1252, 1430, 1098, 1063, 1925, 1048, 2694, 1007, 1576, 1011,\n",
       "       1010, 1285,   18,    3,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b385ac67-7d69-4590-9ae5-301fed4b49c2",
   "metadata": {},
   "source": [
    "The `tokenize` adds a **start** and **end** token to each sequence of text and fills with **pad** tokens to get 280 tokens. The **start** token is 2, **end** is 3, and **pad** is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcb4eac-146a-4bd7-bd4a-df5cf5a0aa32",
   "metadata": {},
   "source": [
    "Now we can input the tweet into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070bf77f-fded-4148-a596-348942457275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 280, 29025)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = model.predict(tweet)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662446fc-0c30-4866-b2a0-2305e2fd8b63",
   "metadata": {},
   "source": [
    "The output is a probability distribution (after you apply softmax) at each token position. To display it we can use `detokenize` but we need to get the most probable token at each position with `np.argmax(p, axis=2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b270168-b0de-4ec9-841e-5884dcb2ecfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tenemos tres dias sin internet ni senal de celular en el pueblo .',\n",
       " 'inc ##om ##un ##ica ##dos en el siglo xxi tampoco hay servicio de que .',\n",
       " 'vamos a comer unos tacos']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.detokenize(np.argmax(p, axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1f334-df1f-4a85-93dc-aa61e02187ce",
   "metadata": {},
   "source": [
    "Note that the tokenizer uses `wordpiece` and could break words like *incomunicados* in different tokens.\n",
    "\n",
    "You can put a **mask** token by changing a token for the number 4. To mask the token for *internet* in the first tweet we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2a2fb9-fdee-4f22-b75d-9aed285b4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet[0][5] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a0194-f5c8-4393-a2d7-cb8820a253f3",
   "metadata": {},
   "source": [
    "Let's now mask the tokens *XXI* and *tacos* from the other tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39fc035-888b-4c9d-929d-d393cd1456ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet[1][9] = 4\n",
    "tweet[2][5] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66acec31-9ecb-4e6e-b4ca-6ab20152df84",
   "metadata": {},
   "source": [
    "And predict the masked tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b04b118-8469-474e-9631-bae69ac88208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tenemos tres dias sin internet ni senal de celular en el pueblo .',\n",
       " 'inc ##om ##un ##ica ##dos en el siglo xxi tampoco hay servicio de que .',\n",
       " 'vamos a comer unos tacos']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = model.predict(tweet)\n",
    "tokenizer.detokenize(np.argmax(p, axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fd40a-3f33-494d-b81d-2c9b87a4e6ca",
   "metadata": {},
   "source": [
    "Note how the masks were correctly predicted.\n",
    "\n",
    "We can also display the top *k* predictions of a position, here, *mask_pos* indicates the positions we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c8e551-8679-4f85-a282-d8e1d8bbd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pos = [5, 9, 5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474af64f-ee95-4ba1-ab92-def24c790deb",
   "metadata": {},
   "source": [
    "Now we can get the top *k* predictions with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "514cbbb3-f843-4d0a-a2e6-bc341ca904ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['internet', 'saber', 'luz', 'tener', 'tomar'],\n",
       " ['xxi', 'pasado', '0', 'que', 'donde'],\n",
       " ['tacos', 'taquitos', 'tamales', 'chilaquiles', 'dias']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.top_k(p, mask_pos, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
